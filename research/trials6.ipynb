{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57828d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Imports & Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "#  Load environment\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "\n",
    "#  Step 1: Load PDF Documents\n",
    "def load_pdf_file(data_path):\n",
    "    loader = DirectoryLoader(data_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    return loader.load()\n",
    "\n",
    "extracted_data = load_pdf_file(\"Data/\")\n",
    "\n",
    "#  Step 2: Split into Chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(extracted_data)\n",
    "print(\"Chunks:\", len(text_chunks))\n",
    "\n",
    "#  Step 3: Download Sentence Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "#  Step 4: Setup Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index_name = \"medicalbot\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "#  Step 5: Push data to Pinecone\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "#  Step 6: Set retriever\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "#  Step 7: Local LLM using HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "class LocalLLMWrapper:\n",
    "    def __init__(self, pipe):\n",
    "        self.pipe = pipe\n",
    "    def invoke(self, prompt):\n",
    "        return self.pipe(prompt, max_new_tokens=100)[0][\"generated_text\"]\n",
    "\n",
    "local_llm = LocalLLMWrapper(pipe)\n",
    "\n",
    "#  Step 8: Build the RAG pipeline\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following retrieved context to answer the question. \"\n",
    "    \"If you don't know the answer, say 'I don't know'. \"\n",
    "    \"Be concise and clear (max 3 sentences).\\n\\n{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(local_llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "#  Step 9: Ask questions\n",
    "questions = [\n",
    "    \"What is Acromegaly and gigantism?\",\n",
    "    \"What is acne?\",\n",
    "    \"What is diabetes?\",\n",
    "    \"What is statistics?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    response = rag_chain.invoke({\"input\": q})\n",
    "    print(f\"\\n Q: {q}\\n A: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    " #output \n",
    " Q: What is Acromegaly and gigantism?\n",
    " A: Acromegaly and gigantism are disorders that involve overproduction of growth hormone...\n",
    "\n",
    " Q: What is statistics?\n",
    " A: I don't know."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
